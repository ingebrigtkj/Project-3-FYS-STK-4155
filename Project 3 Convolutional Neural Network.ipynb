{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d68ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 19.488170019721387\n",
      "Epoch 2/25, Loss: 0.679838683724943\n",
      "Epoch 3/25, Loss: 0.6394124469552938\n",
      "Epoch 4/25, Loss: 0.6083874601037205\n",
      "Epoch 5/25, Loss: 0.5851365955821669\n",
      "Epoch 6/25, Loss: 0.5683662219993432\n",
      "Epoch 7/25, Loss: 0.5570356951402693\n",
      "Epoch 8/25, Loss: 0.550297820059328\n",
      "Epoch 9/25, Loss: 0.5474546292781071\n",
      "Epoch 10/25, Loss: 0.5479243795524879\n",
      "Epoch 11/25, Loss: 0.5512167916127482\n",
      "Epoch 12/25, Loss: 0.5569143668395438\n",
      "Epoch 13/25, Loss: 0.5646582000727851\n",
      "Epoch 14/25, Loss: 0.5741371316747631\n",
      "Epoch 15/25, Loss: 0.5850793897210324\n",
      "Epoch 16/25, Loss: 0.5972460971400335\n",
      "Epoch 17/25, Loss: 0.6104261820035132\n",
      "Epoch 18/25, Loss: 0.6244323486900393\n",
      "Epoch 19/25, Loss: 0.6390978553319215\n",
      "Epoch 20/25, Loss: 0.6542739074858663\n",
      "Epoch 21/25, Loss: 0.6698275256072626\n",
      "Epoch 22/25, Loss: 0.6856397791944416\n",
      "Epoch 23/25, Loss: 0.7016043066944583\n",
      "Epoch 24/25, Loss: 0.7176260598208886\n",
      "Epoch 25/25, Loss: 0.7336202255716335\n"
     ]
    }
   ],
   "source": [
    "#Convolutional Neural Network solving the PDE, Time and Spatial Discretized and turned\n",
    "#into an image, the regular input to a CNN using patches to capture the dimensions of the image.\n",
    "#Also using Weight-Sharing, Gradient Clipping and Learning rates.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Creating patches of the PDE:\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "# Build a simple CNN\n",
    "class SimpleCNN:\n",
    "    #Initialization of the convolutional operation:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "        \n",
    "    #The loss function:\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "    \n",
    "    #The forward pass:\n",
    "    def forward(self, input_data):\n",
    "        return np.dot(self.weights, input_data) + self.bias\n",
    "\n",
    "    #The backward pass:\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = input_data.T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights)\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Weight sharing\n",
    "        dloss_dweights_shared = np.mean(dloss_dweights, axis=0, keepdims=True)\n",
    "        dloss_dweights = np.tile(dloss_dweights_shared, (self.weights.shape[0], 1))\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * dloss_dweights\n",
    "        self.bias -= learning_rate * dloss_dbias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model\n",
    "model = SimpleCNN(input_size=patch_size**2, output_size=1)\n",
    "\n",
    "#Setting the number of epochs and learning rate (Adapting over iterations)\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "#Scaled data\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = np.array(predicted_output_normalized)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Comment: Loss is very good after only 2 iterations, w. the best score at the 9th iteration,\n",
    "#It then diverges and becomes worse over the next 16 iterations. The best score increases by\n",
    "#30% by the 25th iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b7f0b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.4394153652638368\n",
      "Epoch 2/25, Loss: 1.4266099261253322\n",
      "Epoch 3/25, Loss: 1.3513009930620201\n",
      "Epoch 4/25, Loss: 1.295086454795722\n",
      "Epoch 5/25, Loss: 1.2533651906714636\n",
      "Epoch 6/25, Loss: 1.222711230387821\n",
      "Epoch 7/25, Loss: 1.2005558932698448\n",
      "Epoch 8/25, Loss: 1.1849589703036247\n",
      "Epoch 9/25, Loss: 1.1744435469821068\n",
      "Epoch 10/25, Loss: 1.1678763749532426\n",
      "Epoch 11/25, Loss: 1.1643808986515467\n",
      "Epoch 12/25, Loss: 1.1632737405466187\n",
      "Epoch 13/25, Loss: 1.1640180794833335\n",
      "Epoch 14/25, Loss: 1.1661892292817535\n",
      "Epoch 15/25, Loss: 1.1694490584827055\n",
      "Epoch 16/25, Loss: 1.1735268426011438\n",
      "Epoch 17/25, Loss: 1.1782048181538671\n",
      "Epoch 18/25, Loss: 1.1833071917050904\n",
      "Epoch 19/25, Loss: 1.188691703119482\n",
      "Epoch 20/25, Loss: 1.1942430898634908\n",
      "Epoch 21/25, Loss: 1.1998679767980105\n",
      "Epoch 22/25, Loss: 1.2054908435530292\n",
      "Epoch 23/25, Loss: 1.2110508135553657\n",
      "Epoch 24/25, Loss: 1.2164990752689366\n",
      "Epoch 25/25, Loss: 1.2217967944425976\n"
     ]
    }
   ],
   "source": [
    "#Adding dropout:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropout:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.5):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, input_data) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = input_data.T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights)\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Weight sharing\n",
    "        dloss_dweights_shared = np.mean(dloss_dweights, axis=0, keepdims=True)\n",
    "        dloss_dweights = np.tile(dloss_dweights_shared, (self.weights.shape[0], 1))\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * dloss_dweights\n",
    "        self.bias -= learning_rate * dloss_dbias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout\n",
    "model_with_dropout = SimpleCNNWithDropout(input_size=patch_size**2, output_size=1, dropout_rate=1)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = np.array(predicted_output_normalized)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#The loss is actually a bit worse after adding the dropout, it starts off very good,\n",
    "#gets better until the 12th iteration, and then starts to increase(diverge) like was\n",
    "#the case without dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "534582ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.48729598241172123\n",
      "Epoch 2/25, Loss: 0.5182457411377474\n",
      "Epoch 3/25, Loss: 0.5509811652657421\n",
      "Epoch 4/25, Loss: 0.5853691010152491\n",
      "Epoch 5/25, Loss: 0.6212839100601316\n",
      "Epoch 6/25, Loss: 0.6586068559128915\n",
      "Epoch 7/25, Loss: 0.6972255515362321\n",
      "Epoch 8/25, Loss: 0.7370334620912152\n",
      "Epoch 9/25, Loss: 0.777929457317894\n",
      "Epoch 10/25, Loss: 0.8198174085766332\n",
      "Epoch 11/25, Loss: 0.862605826059851\n",
      "Epoch 12/25, Loss: 0.9062075321189945\n",
      "Epoch 13/25, Loss: 0.950539367047202\n",
      "Epoch 14/25, Loss: 0.9955219240142179\n",
      "Epoch 15/25, Loss: 1.0410793101737412\n",
      "Epoch 16/25, Loss: 1.087138931255346\n",
      "Epoch 17/25, Loss: 1.133631297217651\n",
      "Epoch 18/25, Loss: 1.1804898467787222\n",
      "Epoch 19/25, Loss: 1.2276507888551287\n",
      "Epoch 20/25, Loss: 1.2750529591373092\n",
      "Epoch 21/25, Loss: 1.3226376902054169\n",
      "Epoch 22/25, Loss: 1.3703486937490608\n",
      "Epoch 23/25, Loss: 1.4181319535988193\n",
      "Epoch 24/25, Loss: 1.4659356284080354\n",
      "Epoch 25/25, Loss: 1.5137099629404533\n"
     ]
    }
   ],
   "source": [
    "#Adding Max pooling:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        return np.max(x.reshape((x.shape[0] // pool_size, pool_size, x.shape[1])), axis=1)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights)\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Weight sharing\n",
    "        dloss_dweights_shared = np.mean(dloss_dweights, axis=0, keepdims=True)\n",
    "        dloss_dweights = np.tile(dloss_dweights_shared, (self.weights.shape[0], 1))\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * dloss_dweights\n",
    "        self.bias -= learning_rate * dloss_dbias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=1)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = np.array(predicted_output_normalized)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Adding max pooling yields a significant boost in the loss result right off the bat.\n",
    "#But it diverges throughout, and in the end does not give a better loss than before.\n",
    "#The 25th epoch gives a loss 200% worse than the 1st epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd6fb549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.749511290228364\n",
      "Epoch 2/25, Loss: 0.7652322179043924\n",
      "Epoch 3/25, Loss: 0.7807237224738086\n",
      "Epoch 4/25, Loss: 0.79593361646902\n",
      "Epoch 5/25, Loss: 0.8108162241539687\n",
      "Epoch 6/25, Loss: 0.8253318487444063\n",
      "Epoch 7/25, Loss: 0.8394462858655248\n",
      "Epoch 8/25, Loss: 0.853130376964142\n",
      "Epoch 9/25, Loss: 0.8663595976482916\n",
      "Epoch 10/25, Loss: 0.8791136768961307\n",
      "Epoch 11/25, Loss: 0.8913762438292213\n",
      "Epoch 12/25, Loss: 0.9031344993298939\n",
      "Epoch 13/25, Loss: 0.9143789102395556\n",
      "Epoch 14/25, Loss: 0.9251029242324929\n",
      "Epoch 15/25, Loss: 0.9353027037417223\n",
      "Epoch 16/25, Loss: 0.9449768775359936\n",
      "Epoch 17/25, Loss: 0.9541263087241204\n",
      "Epoch 18/25, Loss: 0.9627538781048467\n",
      "Epoch 19/25, Loss: 0.9708642818944061\n",
      "Epoch 20/25, Loss: 0.9784638429577661\n",
      "Epoch 21/25, Loss: 0.9855603347454017\n",
      "Epoch 22/25, Loss: 0.9921628172016976\n",
      "Epoch 23/25, Loss: 0.9982814839641161\n",
      "Epoch 24/25, Loss: 1.0039275202192726\n",
      "Epoch 25/25, Loss: 1.0091129706203046\n",
      "Predicted Output Shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#Increasing the Pool size:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=5):\n",
    "        return np.max(x.reshape((x.shape[0] // pool_size, pool_size, x.shape[1])), axis=1)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights)\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Weight sharing\n",
    "        dloss_dweights_shared = np.mean(dloss_dweights, axis=0, keepdims=True)\n",
    "        dloss_dweights = np.tile(dloss_dweights_shared, (self.weights.shape[0], 1))\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * dloss_dweights\n",
    "        self.bias -= learning_rate * dloss_dbias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=1)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = np.array(predicted_output_normalized)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Increasing the pooling size, the loss starts off a bit worse, but it does not \n",
    "#diverge as quickly as before, and is actually more stable than the smaller pool size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5672d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.97251672624296\n",
      "Epoch 2/25, Loss: 0.9703780722589356\n",
      "Epoch 3/25, Loss: 0.9682743475779226\n",
      "Epoch 4/25, Loss: 0.9662065141956158\n",
      "Epoch 5/25, Loss: 0.9641753289297903\n",
      "Epoch 6/25, Loss: 0.9621813533250158\n",
      "Epoch 7/25, Loss: 0.9602249634050388\n",
      "Epoch 8/25, Loss: 0.9583063592595606\n",
      "Epoch 9/25, Loss: 0.9564255744532689\n",
      "Epoch 10/25, Loss: 0.9545824852467795\n",
      "Epoch 11/25, Loss: 0.9527768196202789\n",
      "Epoch 12/25, Loss: 0.951008166091551\n",
      "Epoch 13/25, Loss: 0.949275982321705\n",
      "Epoch 14/25, Loss: 0.9475796035028147\n",
      "Epoch 15/25, Loss: 0.9459182505225302\n",
      "Epoch 16/25, Loss: 0.9442910379014717\n",
      "Epoch 17/25, Loss: 0.9426969815006442\n",
      "Epoch 18/25, Loss: 0.9411350059964566\n",
      "Epoch 19/25, Loss: 0.9396039521215985\n",
      "Epoch 20/25, Loss: 0.9381025836709352\n",
      "Epoch 21/25, Loss: 0.9366295942723087\n",
      "Epoch 22/25, Loss: 0.9351836139221827\n",
      "Epoch 23/25, Loss: 0.933763215287155\n",
      "Epoch 24/25, Loss: 0.9323669197726809\n",
      "Epoch 25/25, Loss: 0.9309932033603368\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 149\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Generate test data (replace this with your actual test data)\u001b[39;00m\n\u001b[0;32m    148\u001b[0m test_input \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((patch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 149\u001b[0m predicted_output_normalized \u001b[38;5;241m=\u001b[39m model_with_dropout_and_max_pooling\u001b[38;5;241m.\u001b[39mforward(test_input, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Denormalize the predicted output\u001b[39;00m\n\u001b[0;32m    152\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m predicted_output_normalized \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(output_data) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(output_data)\n",
      "Cell \u001b[1;32mIn[42], line 91\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.forward\u001b[1;34m(self, input_data, training)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(np\u001b[38;5;241m.\u001b[39mones_like(input_data))\n\u001b[0;32m     89\u001b[0m     input_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pooling(input_data)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Adding momentum and l2-regularization:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1, momentum=1.7, l2_lambda=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.momentum = momentum\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "        # Momentum terms for weights and bias\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.v_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        pool_height = x.shape[0] // pool_size\n",
    "        pool_width = pool_size\n",
    "        return np.max(x[:pool_height * pool_size, :].reshape((pool_height, pool_width, x.shape[1])), axis=1)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2) + 0.5 * self.l2_lambda * np.sum(self.weights ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights) + self.l2_lambda * self.weights\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Update momentum terms\n",
    "        self.v_weights = self.momentum * self.v_weights - learning_rate * dloss_dweights\n",
    "        self.v_bias = self.momentum * self.v_bias - learning_rate * dloss_dbias\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights += self.v_weights\n",
    "        self.bias += self.v_bias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=0.5, momentum=0.9, l2_lambda=0.001)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = model_with_dropout_and_max_pooling.forward(test_input, training=False)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#A dimension error here, but it is not relevant for the calculation of loss.\n",
    "#This time, the results are not necessarily better than the previous result,\n",
    "#but after adding momentum and l2-regularization, the loss is more stable,\n",
    "#and it converges, like I want it to. The mean loss over the 25 iterations is however equal to the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d529f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.0291087828494012\n",
      "Epoch 2/25, Loss: 1.0272175113295872\n",
      "Epoch 3/25, Loss: 1.0252411260393353\n",
      "Epoch 4/25, Loss: 1.0231888614179032\n",
      "Epoch 5/25, Loss: 1.0210695338293556\n",
      "Epoch 6/25, Loss: 1.0188915439409618\n",
      "Epoch 7/25, Loss: 1.0166628801691322\n",
      "Epoch 8/25, Loss: 1.0143911230926639\n",
      "Epoch 9/25, Loss: 1.0120834507388925\n",
      "Epoch 10/25, Loss: 1.0097466446550698\n",
      "Epoch 11/25, Loss: 1.0073870966816147\n",
      "Epoch 12/25, Loss: 1.0050108163504383\n",
      "Epoch 13/25, Loss: 1.0026234388355\n",
      "Epoch 14/25, Loss: 1.000230233388299\n",
      "Epoch 15/25, Loss: 0.9978361121946466\n",
      "Epoch 16/25, Loss: 0.995445639594898\n",
      "Epoch 17/25, Loss: 0.9930630416117012\n",
      "Epoch 18/25, Loss: 0.9906922157350581\n",
      "Epoch 19/25, Loss: 0.9883367409171913\n",
      "Epoch 20/25, Loss: 0.98599988773353\n",
      "Epoch 21/25, Loss: 0.9836846286689237\n",
      "Epoch 22/25, Loss: 0.9813936484916685\n",
      "Epoch 23/25, Loss: 0.9791293546810003\n",
      "Epoch 24/25, Loss: 0.9768938878756134\n",
      "Epoch 25/25, Loss: 0.9746891323145056\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 25 into shape (12,2,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Generate test data (replace this with your actual test data)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m test_input \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((patch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 147\u001b[0m predicted_output_normalized \u001b[38;5;241m=\u001b[39m model_with_dropout_and_max_pooling\u001b[38;5;241m.\u001b[39mforward(test_input, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Denormalize the predicted output\u001b[39;00m\n\u001b[0;32m    150\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m predicted_output_normalized \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(output_data) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(output_data)\n",
      "Cell \u001b[1;32mIn[41], line 89\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.forward\u001b[1;34m(self, input_data, training)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(np\u001b[38;5;241m.\u001b[39mones_like(input_data))\n\u001b[0;32m     87\u001b[0m     input_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pooling(input_data)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "Cell \u001b[1;32mIn[41], line 79\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.max_pooling\u001b[1;34m(self, x, pool_size)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_pooling\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(x\u001b[38;5;241m.\u001b[39mreshape((x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m pool_size, pool_size, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 25 into shape (12,2,1)"
     ]
    }
   ],
   "source": [
    "#Increasing the momentum:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1, momentum=1.7, l2_lambda=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.momentum = momentum\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "        # Momentum terms for weights and bias\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.v_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        return np.max(x.reshape((x.shape[0] // pool_size, pool_size, x.shape[1])), axis=1)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2) + 0.5 * self.l2_lambda * np.sum(self.weights ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights) + self.l2_lambda * self.weights\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Update momentum terms\n",
    "        self.v_weights = self.momentum * self.v_weights - learning_rate * dloss_dweights\n",
    "        self.v_bias = self.momentum * self.v_bias - learning_rate * dloss_dbias\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights += self.v_weights\n",
    "        self.bias += self.v_bias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=0.5, momentum=0.9, l2_lambda=0.001)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = model_with_dropout_and_max_pooling.forward(test_input, training=False)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Again the dimension error is irrelevant for the loss computation. Increasing the momentum\n",
    "#did not improve the Loss, it made it 10% worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8520d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.9296405022162872\n",
      "Epoch 2/25, Loss: 0.9283072180730906\n",
      "Epoch 3/25, Loss: 0.9269917233876318\n",
      "Epoch 4/25, Loss: 0.9256923662786471\n",
      "Epoch 5/25, Loss: 0.9244074752468087\n",
      "Epoch 6/25, Loss: 0.9231353636812872\n",
      "Epoch 7/25, Loss: 0.9218743341566021\n",
      "Epoch 8/25, Loss: 0.9206226825235528\n",
      "Epoch 9/25, Loss: 0.9193787017988466\n",
      "Epoch 10/25, Loss: 0.918140685857332\n",
      "Epoch 11/25, Loss: 0.9169069329316383\n",
      "Epoch 12/25, Loss: 0.91567574892357\n",
      "Epoch 13/25, Loss: 0.9144454505320273\n",
      "Epoch 14/25, Loss: 0.9132143682021762\n",
      "Epoch 15/25, Loss: 0.9119808489005875\n",
      "Epoch 16/25, Loss: 0.9107432587211063\n",
      "Epoch 17/25, Loss: 0.9094999853265869\n",
      "Epoch 18/25, Loss: 0.9082494402310476\n",
      "Epoch 19/25, Loss: 0.9069900609270625\n",
      "Epoch 20/25, Loss: 0.9057203128636879\n",
      "Epoch 21/25, Loss: 0.9044386912792378\n",
      "Epoch 22/25, Loss: 0.9031437228940355\n",
      "Epoch 23/25, Loss: 0.9018339674678587\n",
      "Epoch 24/25, Loss: 0.9005080192267513\n",
      "Epoch 25/25, Loss: 0.8991645081638854\n"
     ]
    }
   ],
   "source": [
    "#Decreasing the Pool size:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "    \n",
    "    #Implementation of dropout:\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "    \n",
    "    #The max pooling convolution:\n",
    "    def max_pooling(self, x, pool_size=1):\n",
    "        return np.max(x.reshape((x.shape[0] // pool_size, pool_size, x.shape[1])), axis=1)\n",
    "    \n",
    "    #Loss function:\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "        \n",
    "    #Calculation using Dot product\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "    \n",
    "    #The backward pass:\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights)\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Weight sharing\n",
    "        dloss_dweights_shared = np.mean(dloss_dweights, axis=0, keepdims=True)\n",
    "        dloss_dweights = np.tile(dloss_dweights_shared, (self.weights.shape[0], 1))\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= learning_rate * dloss_dweights\n",
    "        self.bias -= learning_rate * dloss_dbias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=1)\n",
    "\n",
    "#Set the no. of epochs and the learning rate:\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = np.array(predicted_output_normalized)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Decreasing the pool size to 1 again yielded more stable results, similar to the addition of \n",
    "#momentum and l2-regularization. It now converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aff78f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.600433786756424\n",
      "Epoch 2/25, Loss: 0.5963356805645813\n",
      "Epoch 3/25, Loss: 0.5922357284472378\n",
      "Epoch 4/25, Loss: 0.5881345516859566\n",
      "Epoch 5/25, Loss: 0.5840327647775346\n",
      "Epoch 6/25, Loss: 0.5799309752660632\n",
      "Epoch 7/25, Loss: 0.5758297835840581\n",
      "Epoch 8/25, Loss: 0.5717297829026802\n",
      "Epoch 9/25, Loss: 0.5676315589908633\n",
      "Epoch 10/25, Loss: 0.5635356900828887\n",
      "Epoch 11/25, Loss: 0.559442746754543\n",
      "Epoch 12/25, Loss: 0.5553532918073663\n",
      "Epoch 13/25, Loss: 0.5512678801610933\n",
      "Epoch 14/25, Loss: 0.5471870587537181\n",
      "Epoch 15/25, Loss: 0.5431113664493721\n",
      "Epoch 16/25, Loss: 0.5390413339535266\n",
      "Epoch 17/25, Loss: 0.5349774837354803\n",
      "Epoch 18/25, Loss: 0.5309203299579555\n",
      "Epoch 19/25, Loss: 0.5268703784134005\n",
      "Epoch 20/25, Loss: 0.522828126467152\n",
      "Epoch 21/25, Loss: 0.5187940630069393\n",
      "Epoch 22/25, Loss: 0.5147686683988082\n",
      "Epoch 23/25, Loss: 0.5107524144490162\n",
      "Epoch 24/25, Loss: 0.5067457643720673\n",
      "Epoch 25/25, Loss: 0.5027491727642819\n"
     ]
    }
   ],
   "source": [
    "#Using 3 max pooling layers/filter instead of 1, pool size = 2:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    #Initialization:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "        \n",
    "    #Dropout function:\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "    \n",
    "    #Max Pooling functionality:\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        pool_height = x.shape[0] // pool_size\n",
    "        pool_width = pool_size\n",
    "\n",
    "        # First max pooling layer\n",
    "        x_pooled = np.max(x[:pool_height * pool_size, :].reshape((pool_height, pool_width, x.shape[1])), axis=1)\n",
    "\n",
    "        # Second max pooling layer\n",
    "        pool_height = x_pooled.shape[0] // pool_size\n",
    "        x_pooled = np.max(x_pooled[:pool_height * pool_size, :].reshape((pool_height, pool_width, x_pooled.shape[1])), axis=1)\n",
    "\n",
    "        # Third max pooling layer\n",
    "        pool_height = x_pooled.shape[0] // pool_size\n",
    "        x_pooled = np.max(x_pooled[:pool_height * pool_size, :].reshape((pool_height, pool_width, x_pooled.shape[1])), axis=1)\n",
    "        \n",
    "        #Return layers within the function.\n",
    "        return x_pooled\n",
    "    \n",
    "    #The loss function\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    #The forward pass\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "        \n",
    "        #Calculation using Dot product, a mathematically expensive operation and can be difficult for huge datasets\n",
    "        #(Can be improved for instance, \n",
    "        #by using inception layers)\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "    \n",
    "    #The backward pass\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias for the dense layer\n",
    "        dpredictions_dweights_2 = x_pooled_flat.T\n",
    "        dpredictions_dbias_2 = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias for the dense layer\n",
    "        dloss_dweights_2 = np.dot(dloss_dpredictions, dpredictions_dweights_2)\n",
    "        dloss_dbias_2 = np.sum(dloss_dpredictions * dpredictions_dbias_2)\n",
    "\n",
    "        # Update weights and bias for the dense layer\n",
    "        self.weights_2 -= learning_rate * dloss_dweights_2\n",
    "        self.bias_2 -= learning_rate * dloss_dbias_2\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=1)\n",
    "\n",
    "#No. of epochs/iterations and learning rate(the learning rate changes along the epochs)\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "#Scaled data:\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop(Using the patches for the images(Transformed PDE)):\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = np.array(predicted_output_normalized)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Adding 2 max pooling layers to the previous 1 max pooling layer greatly improved the loss,\n",
    "#it is stable, and it also converges, providing the best result yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95e32e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 98564.44620622172\n",
      "Epoch 2/25, Loss: 39570.975096831105\n",
      "Epoch 3/25, Loss: 12134.772058319404\n",
      "Epoch 4/25, Loss: 9649.025408429445\n",
      "Epoch 5/25, Loss: 10894.058208341317\n",
      "Epoch 6/25, Loss: 11604.287538487282\n",
      "Epoch 7/25, Loss: 11353.166929911938\n",
      "Epoch 8/25, Loss: 10436.131660783958\n",
      "Epoch 9/25, Loss: 9221.865531259788\n",
      "Epoch 10/25, Loss: 8163.630310351647\n",
      "Epoch 11/25, Loss: 7258.14443503094\n",
      "Epoch 12/25, Loss: 6455.5541170265715\n",
      "Epoch 13/25, Loss: 5723.898785736836\n",
      "Epoch 14/25, Loss: 4964.957139805589\n",
      "Epoch 15/25, Loss: 4133.616252927568\n",
      "Epoch 16/25, Loss: 3455.054988284962\n",
      "Epoch 17/25, Loss: 2945.080779672834\n",
      "Epoch 18/25, Loss: 2502.182429920468\n",
      "Epoch 19/25, Loss: 2141.5176662126746\n",
      "Epoch 20/25, Loss: 1850.3241020615844\n",
      "Epoch 21/25, Loss: 1608.5537158620928\n",
      "Epoch 22/25, Loss: 1415.4844551535405\n",
      "Epoch 23/25, Loss: 1265.577545298806\n",
      "Epoch 24/25, Loss: 1147.6629304151238\n",
      "Epoch 25/25, Loss: 1063.8762502199884\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 148\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Generate test data (replace this with your actual test data)\u001b[39;00m\n\u001b[0;32m    147\u001b[0m test_input \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((patch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 148\u001b[0m predicted_output_normalized \u001b[38;5;241m=\u001b[39m model_with_dropout_and_max_pooling\u001b[38;5;241m.\u001b[39mforward(test_input, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Denormalize the predicted output\u001b[39;00m\n\u001b[0;32m    151\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m predicted_output_normalized \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(output_data) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(output_data)\n",
      "Cell \u001b[1;32mIn[106], line 90\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.forward\u001b[1;34m(self, input_data, training)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(np\u001b[38;5;241m.\u001b[39mones_like(input_data))\n\u001b[0;32m     88\u001b[0m     input_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pooling(input_data)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Greatly decreasing the l2-regularization:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1, momentum=0.9, l2_lambda=0.000001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.momentum = momentum\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "        # Momentum terms for weights and bias\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.v_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        pool_height = x.shape[0] // pool_size\n",
    "        pool_width = pool_size\n",
    "        return np.max(x[:pool_height * pool_size, :].reshape((pool_height, pool_width, x.shape[1])), axis=1)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2) + 0.5 * self.l2_lambda * np.sum(self.weights ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights) + self.l2_lambda * self.weights\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Update momentum terms\n",
    "        self.v_weights = self.momentum * self.v_weights - learning_rate * dloss_dweights\n",
    "        self.v_bias = self.momentum * self.v_bias - learning_rate * dloss_dbias\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights += self.v_weights\n",
    "        self.bias += self.v_bias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=0.5, momentum=0.9, l2_lambda=0.000001)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = model_with_dropout_and_max_pooling.forward(test_input, training=False)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Decreasing the l2-regularization, the loss is very bad,\n",
    "#and it does not converge strictly, like I want it to. It does however converge in a sense, but the end result loss\n",
    "#is not good and it indicates I need a lot of epochs to generate a good result here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94fd8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1005.2374234536366\n",
      "Epoch 2/25, Loss: 967.3832537982488\n",
      "Epoch 3/25, Loss: 945.4262134185143\n",
      "Epoch 4/25, Loss: 932.9281987451906\n",
      "Epoch 5/25, Loss: 932.2274856857196\n",
      "Epoch 6/25, Loss: 940.1565740918371\n",
      "Epoch 7/25, Loss: 954.1817479693354\n",
      "Epoch 8/25, Loss: 974.2492770814231\n",
      "Epoch 9/25, Loss: 998.7311759692318\n",
      "Epoch 10/25, Loss: 1022.8571783356274\n",
      "Epoch 11/25, Loss: 1045.8708758070413\n",
      "Epoch 12/25, Loss: 1068.029642581275\n",
      "Epoch 13/25, Loss: 1091.0016945262291\n",
      "Epoch 14/25, Loss: 1115.4298857043357\n",
      "Epoch 15/25, Loss: 1139.7372746373844\n",
      "Epoch 16/25, Loss: 1165.719535417762\n",
      "Epoch 17/25, Loss: 1197.2019460716003\n",
      "Epoch 18/25, Loss: 1221.2320093860035\n",
      "Epoch 19/25, Loss: 1235.9366488636279\n",
      "Epoch 20/25, Loss: 1244.1468261945095\n",
      "Epoch 21/25, Loss: 1246.3998375665246\n",
      "Epoch 22/25, Loss: 1244.2529111985098\n",
      "Epoch 23/25, Loss: 1238.4417828534029\n",
      "Epoch 24/25, Loss: 1230.1825531981187\n",
      "Epoch 25/25, Loss: 1219.758326992487\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 149\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Generate test data (replace this with your actual test data)\u001b[39;00m\n\u001b[0;32m    148\u001b[0m test_input \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((patch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 149\u001b[0m predicted_output_normalized \u001b[38;5;241m=\u001b[39m model_with_dropout_and_max_pooling\u001b[38;5;241m.\u001b[39mforward(test_input, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Denormalize the predicted output\u001b[39;00m\n\u001b[0;32m    152\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m predicted_output_normalized \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(output_data) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(output_data)\n",
      "Cell \u001b[1;32mIn[107], line 91\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.forward\u001b[1;34m(self, input_data, training)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(np\u001b[38;5;241m.\u001b[39mones_like(input_data))\n\u001b[0;32m     89\u001b[0m     input_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pooling(input_data)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Greatly increasing the l2-regularization:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1, momentum=0.9, l2_lambda=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.momentum = momentum\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "        # Momentum terms for weights and bias\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.v_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        pool_height = x.shape[0] // pool_size\n",
    "        pool_width = pool_size\n",
    "        return np.max(x[:pool_height * pool_size, :].reshape((pool_height, pool_width, x.shape[1])), axis=1)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2) + 0.5 * self.l2_lambda * np.sum(self.weights ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights) + self.l2_lambda * self.weights\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Update momentum terms\n",
    "        self.v_weights = self.momentum * self.v_weights - learning_rate * dloss_dweights\n",
    "        self.v_bias = self.momentum * self.v_bias - learning_rate * dloss_dbias\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights += self.v_weights\n",
    "        self.bias += self.v_bias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=0.5, momentum=0.9, l2_lambda=1)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = model_with_dropout_and_max_pooling.forward(test_input, training=False)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Increasing the l2-regularization, the loss is very bad,\n",
    "#and it does not converge strictly, like I want it to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "85752d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1208.088075130057\n",
      "Epoch 2/25, Loss: 1195.6533056604505\n",
      "Epoch 3/25, Loss: 1182.8458152437122\n",
      "Epoch 4/25, Loss: 1169.9976510971687\n",
      "Epoch 5/25, Loss: 1157.1996003295371\n",
      "Epoch 6/25, Loss: 1144.5912353351662\n",
      "Epoch 7/25, Loss: 1132.1301236550828\n",
      "Epoch 8/25, Loss: 1119.8875592177794\n",
      "Epoch 9/25, Loss: 1107.8895488001372\n",
      "Epoch 10/25, Loss: 1096.2993079596483\n",
      "Epoch 11/25, Loss: 1085.349352564321\n",
      "Epoch 12/25, Loss: 1074.7951719984633\n",
      "Epoch 13/25, Loss: 1064.382785171043\n",
      "Epoch 14/25, Loss: 1055.5450811964201\n",
      "Epoch 15/25, Loss: 1047.391072306418\n",
      "Epoch 16/25, Loss: 1039.7697020053254\n",
      "Epoch 17/25, Loss: 1032.5592401912597\n",
      "Epoch 18/25, Loss: 1025.670122028968\n",
      "Epoch 19/25, Loss: 1019.3520714396892\n",
      "Epoch 20/25, Loss: 1013.4411985020475\n",
      "Epoch 21/25, Loss: 1007.9268046849604\n",
      "Epoch 22/25, Loss: 1002.7581169558804\n",
      "Epoch 23/25, Loss: 997.9302629118462\n",
      "Epoch 24/25, Loss: 993.4143791987491\n",
      "Epoch 25/25, Loss: 989.5264526295529\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 149\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Generate test data (replace this with your actual test data)\u001b[39;00m\n\u001b[0;32m    148\u001b[0m test_input \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((patch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 149\u001b[0m predicted_output_normalized \u001b[38;5;241m=\u001b[39m model_with_dropout_and_max_pooling\u001b[38;5;241m.\u001b[39mforward(test_input, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Denormalize the predicted output\u001b[39;00m\n\u001b[0;32m    152\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m predicted_output_normalized \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(output_data) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(output_data)\n",
      "Cell \u001b[1;32mIn[108], line 91\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.forward\u001b[1;34m(self, input_data, training)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(np\u001b[38;5;241m.\u001b[39mones_like(input_data))\n\u001b[0;32m     89\u001b[0m     input_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pooling(input_data)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,25) and (12,1) not aligned: 25 (dim 1) != 12 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Greatly increasing the l2-regularization:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    for i in range(1, Nx-1):\n",
    "        u[i, j+1] = u[i, j] + dt / (2 * dx**2) * (u[i+1, j] - 2*u[i, j] + u[i-1, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1, momentum=0.9, l2_lambda=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.momentum = momentum\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "        # Momentum terms for weights and bias\n",
    "        self.v_weights = np.zeros_like(self.weights)\n",
    "        self.v_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        pool_height = x.shape[0] // pool_size\n",
    "        pool_width = pool_size\n",
    "        return np.max(x[:pool_height * pool_size, :].reshape((pool_height, pool_width, x.shape[1])), axis=1)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2) + 0.5 * self.l2_lambda * np.sum(self.weights ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias\n",
    "        dpredictions_dweights = self.max_pooling(input_data).T\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias\n",
    "        dloss_dweights = np.dot(dloss_dpredictions, dpredictions_dweights) + self.l2_lambda * self.weights\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        # Gradient clipping\n",
    "        grad_clip = 10.0\n",
    "        dloss_dweights = np.clip(dloss_dweights, -grad_clip, grad_clip)\n",
    "        dloss_dbias = np.clip(dloss_dbias, -grad_clip, grad_clip)\n",
    "\n",
    "        # Update momentum terms\n",
    "        self.v_weights = self.momentum * self.v_weights - learning_rate * dloss_dweights\n",
    "        self.v_bias = self.momentum * self.v_bias - learning_rate * dloss_dbias\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights += self.v_weights\n",
    "        self.bias += self.v_bias\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=0.5, momentum=0.9, l2_lambda=0.001)\n",
    "\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data (replace this with your actual test data)\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = model_with_dropout_and_max_pooling.forward(test_input, training=False)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Increasing the l2-regularization, the loss is very bad,\n",
    "#and it does not converge strictly, like I want it to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f08d8c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kjreng\\AppData\\Local\\Temp\\ipykernel_6180\\1061678538.py:72: RuntimeWarning: divide by zero encountered in divide\n",
      "  mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,25) and (3,1) not aligned: 25 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 135\u001b[0m\n\u001b[0;32m    132\u001b[0m     input_patch \u001b[38;5;241m=\u001b[39m input_data_normalized[i]\u001b[38;5;241m.\u001b[39mreshape((patch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    133\u001b[0m     output_patch \u001b[38;5;241m=\u001b[39m output_data_normalized[i]\n\u001b[1;32m--> 135\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model_with_dropout_and_max_pooling\u001b[38;5;241m.\u001b[39mbackward(input_patch, output_patch, learning_rate)\n\u001b[0;32m    136\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[111], line 100\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.backward\u001b[1;34m(self, input_data, output_data, learning_rate)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data, output_data, learning_rate):\n\u001b[1;32m--> 100\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(input_data, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(predictions, output_data)\n\u001b[0;32m    103\u001b[0m     dloss_dpredictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (predictions \u001b[38;5;241m-\u001b[39m output_data) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(output_data)\n",
      "Cell \u001b[1;32mIn[111], line 97\u001b[0m, in \u001b[0;36mSimpleCNNWithDropoutAndMaxPooling.forward\u001b[1;34m(self, input_data, training)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(np\u001b[38;5;241m.\u001b[39mones_like(input_data))\n\u001b[0;32m     95\u001b[0m     input_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pooling(input_data, pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,25) and (3,1) not aligned: 25 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    # Solve using Implicit Euler's Method\n",
    "    A = np.eye(Nx) + dt / (dx**2) * (-2 * np.eye(Nx, k=0) + np.eye(Nx, k=1) + np.eye(Nx, k=-1))\n",
    "    A[0, 0] = 1.0\n",
    "    A[-1, -1] = 1.0\n",
    "\n",
    "    u[:, j+1] = np.linalg.solve(A, u[:, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        pool_height = x.shape[0] // pool_size\n",
    "        pool_width = pool_size\n",
    "\n",
    "        x_pooled = np.max(x[:pool_height * pool_size, :].reshape((pool_height, pool_width, x.shape[1])), axis=1)\n",
    "\n",
    "        pool_height = x_pooled.shape[0] // pool_size\n",
    "        x_pooled = np.max(x_pooled[:pool_height * pool_size, :].reshape((pool_height, pool_width, x_pooled.shape[1])), axis=1)\n",
    "\n",
    "        pool_height = x_pooled.shape[0] // pool_size\n",
    "        x_pooled = np.max(x_pooled[:pool_height * pool_size, :].reshape((pool_height, pool_width, x_pooled.shape[1])), axis=1)\n",
    "\n",
    "        return x_pooled\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "\n",
    "        return np.dot(self.weights, self.max_pooling(input_data, pool_size=2).flatten().reshape(-1, 1)) + self.bias\n",
    "\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "        dpredictions_dweights = self.max_pooling(input_data, pool_size=2).flatten()\n",
    "        dpredictions_dbias = 1.0\n",
    "\n",
    "        dloss_dweights = np.outer(dloss_dpredictions, dpredictions_dweights)\n",
    "        dloss_dbias = np.sum(dloss_dpredictions * dpredictions_dbias)\n",
    "\n",
    "        self.weights -= learning_rate * dloss_dweights\n",
    "        self.bias -= learning_rate * dloss_dbias\n",
    "\n",
    "        return loss\n",
    "    \n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=1)\n",
    "\n",
    "# No. of epochs/iterations and learning rate (the learning rate changes along the epochs)\n",
    "epochs = 25\n",
    "learning_rate = 0.1 / (epoch + 1)\n",
    "\n",
    "# Scaled data\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop (Using the patches for the images(Transformed PDE)):\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "\n",
    "        loss = model_with_dropout_and_max_pooling.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = model_with_dropout_and_max_pooling.forward(test_input, training=False)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba64a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 4012471.4883261328\n"
     ]
    }
   ],
   "source": [
    "#Using 3 max pooling layers/filter instead of 1, pool size = 2:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "L = 1.0\n",
    "T = 1.0\n",
    "Nx = 100  # Number of spatial points\n",
    "Nt = 1000  # Number of time steps\n",
    "dx = L / (Nx - 1)\n",
    "dt = T / (10 * Nt)\n",
    "\n",
    "# Initial condition\n",
    "def initial_condition(x):\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "# Boundary conditions\n",
    "def boundary_conditions(u):\n",
    "    u[0] = 0.0\n",
    "    u[-1] = 0.0\n",
    "\n",
    "# Generate training data\n",
    "x_values = np.linspace(0, L, Nx)\n",
    "t_values = np.linspace(0, T, Nt+1)\n",
    "\n",
    "u_init = initial_condition(x_values)\n",
    "u = np.zeros((Nx, Nt+1))\n",
    "u[:, 0] = u_init\n",
    "\n",
    "alpha = 0.1  # Adjust the alpha value as needed\n",
    "\n",
    "for j in range(0, Nt):\n",
    "    u[1:-1, j + 1] = u[1:-1, j] + alpha * (u[2:, j] - 2 * u[1:-1, j] + u[:-2, j])\n",
    "\n",
    "    # Apply boundary conditions\n",
    "    boundary_conditions(u[:, j+1])\n",
    "\n",
    "# Create image-like patches\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "patch_size = 5  # Adjust the patch size as needed\n",
    "\n",
    "for j in range(Nt):\n",
    "    for i in range(Nx - patch_size + 1):\n",
    "        input_patch = u[i:i+patch_size, j:j+patch_size].flatten()\n",
    "        output_patch = u[i+patch_size//2, j+patch_size//2] if i+patch_size//2 < Nx and j+patch_size//2 < Nt else 0.0\n",
    "\n",
    "        # Pad smaller patches\n",
    "        if len(input_patch) < patch_size**2:\n",
    "            input_patch = np.pad(input_patch, (0, patch_size**2 - len(input_patch)))\n",
    "\n",
    "        input_data.append(input_patch)\n",
    "        output_data.append(output_patch)\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data).reshape(-1, 1)\n",
    "\n",
    "\n",
    "class SimpleCNNWithDropoutAndMaxPooling:\n",
    "    #Initialization:\n",
    "    def __init__(self, input_size, output_size, dropout_rate=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Use He/Glorot initialization for weights\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / (input_size + output_size))\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "        \n",
    "    #Dropout function:\n",
    "    def dropout(self, x):\n",
    "        mask = (np.random.rand(*x.shape) < self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return x * mask\n",
    "    \n",
    "    #Max Pooling functionality:\n",
    "    def max_pooling(self, x, pool_size=2):\n",
    "        pool_height = x.shape[0] // pool_size\n",
    "        pool_width = pool_size\n",
    "\n",
    "        # First max pooling layer\n",
    "        x_pooled = np.max(x[:pool_height * pool_size, :].reshape((pool_height, pool_width, x.shape[1])), axis=1)\n",
    "\n",
    "        # Second max pooling layer\n",
    "        pool_height = x_pooled.shape[0] // pool_size\n",
    "        x_pooled = np.max(x_pooled[:pool_height * pool_size, :].reshape((pool_height, pool_width, x_pooled.shape[1])), axis=1)\n",
    "\n",
    "        # Third max pooling layer\n",
    "        pool_height = x_pooled.shape[0] // pool_size\n",
    "        x_pooled = np.max(x_pooled[:pool_height * pool_size, :].reshape((pool_height, pool_width, x_pooled.shape[1])), axis=1)\n",
    "        \n",
    "        #Return layers within the function.\n",
    "        return x_pooled\n",
    "    \n",
    "    #The loss function\n",
    "    def loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    #The forward pass\n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = self.dropout(np.ones_like(input_data))\n",
    "            input_data *= self.mask\n",
    "        \n",
    "        #Calculation using Dot product, a mathematically expensive operation and can be difficult for huge datasets\n",
    "        #(Can be improved for instance, \n",
    "        #by using inception layers)\n",
    "        return np.dot(self.weights, self.max_pooling(input_data)) + self.bias\n",
    "    \n",
    "    #The backward pass\n",
    "    def backward(self, input_data, output_data, learning_rate):\n",
    "        predictions = self.forward(input_data, training=True)\n",
    "        loss = self.loss(predictions, output_data)\n",
    "\n",
    "        # Gradient of loss with respect to predictions\n",
    "        dloss_dpredictions = 2 * (predictions - output_data) / len(output_data)\n",
    "\n",
    "        # Gradient of predictions with respect to weights and bias for the dense layer\n",
    "        dpredictions_dweights_2 = x_pooled_flat.T\n",
    "        dpredictions_dbias_2 = 1.0\n",
    "\n",
    "        # Gradients of loss with respect to weights and bias for the dense layer\n",
    "        dloss_dweights_2 = np.dot(dloss_dpredictions, dpredictions_dweights_2)\n",
    "        dloss_dbias_2 = np.sum(dloss_dpredictions * dpredictions_dbias_2)\n",
    "\n",
    "        # Update weights and bias for the dense layer\n",
    "        self.weights_2 -= learning_rate * dloss_dweights_2\n",
    "        self.bias_2 -= learning_rate * dloss_dbias_2\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Train the model with dropout and max pooling\n",
    "model_with_dropout_and_max_pooling = SimpleCNNWithDropoutAndMaxPooling(input_size=patch_size**2, output_size=1, dropout_rate=1)\n",
    "\n",
    "#No. of epochs/iterations and learning rate(the learning rate changes along the epochs)\n",
    "epochs = 25\n",
    "learning_rate = 0.00001 / (epoch + 1)\n",
    "\n",
    "#Scaled data:\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "# Normalize the input and output data\n",
    "input_data_normalized = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "output_data_normalized = (output_data - np.mean(output_data)) / np.std(output_data)\n",
    "\n",
    "# Training loop(Using the patches for the images(Transformed PDE)):\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(input_data_normalized)):\n",
    "        input_patch = input_data_normalized[i].reshape((patch_size**2, 1))\n",
    "        output_patch = output_data_normalized[i]\n",
    "        \n",
    "        loss = model.backward(input_patch, output_patch, learning_rate)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Generate test data\n",
    "test_input = input_data[0].reshape((patch_size**2, 1))\n",
    "predicted_output_normalized = np.array(predicted_output_normalized)\n",
    "\n",
    "# Denormalize the predicted output\n",
    "predicted_output = predicted_output_normalized * np.std(output_data) + np.mean(output_data)\n",
    "\n",
    "#Adding 2 max pooling layers to the previous 1 max pooling layer greatly improved the loss,\n",
    "#it is stable, and it also converges, providing the best result yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880094d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
